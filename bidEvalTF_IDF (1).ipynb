{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# BIDDER EVALUATION SYSTEM - Google Colab Version\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Step 1: Generate Sample Bidder Data\n",
        "np.random.seed(42)\n",
        "\n",
        "bidders = [f\"Bidder_{i+1}\" for i in range(15)]\n",
        "data = {\n",
        "    \"Bidder\": bidders,\n",
        "    \"Cost\": np.random.randint(100000, 500000, size=len(bidders)),\n",
        "    \"Experience\": np.random.randint(1, 10, size=len(bidders)),  # in years\n",
        "    \"QualityScore\": np.random.randint(60, 100, size=len(bidders)),  # out of 100\n",
        "    \"DeliveryTime\": np.random.randint(15, 60, size=len(bidders))  # in days\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"=== Raw Bidder Data ===\")\n",
        "print(df)\n",
        "\n",
        "# Step 2: Define Evaluation Weights\n",
        "# Lower cost and delivery time is better. Higher experience and quality is better.\n",
        "weights = {\n",
        "    \"Cost\": 0.4,\n",
        "    \"Experience\": 0.2,\n",
        "    \"QualityScore\": 0.3,\n",
        "    \"DeliveryTime\": 0.1\n",
        "}\n",
        "\n",
        "print(\"\\n=== Evaluation Criteria Weights ===\")\n",
        "print(weights)\n",
        "\n",
        "# Step 3: Normalize Data for Fair Comparison\n",
        "df_normalized = df.copy()\n",
        "\n",
        "# Normalize cost and delivery time by min-max (lower is better ‚Üí inverse)\n",
        "df_normalized[\"Cost\"] = 1 - (df[\"Cost\"] - df[\"Cost\"].min()) / (df[\"Cost\"].max() - df[\"Cost\"].min())\n",
        "df_normalized[\"DeliveryTime\"] = 1 - (df[\"DeliveryTime\"] - df[\"DeliveryTime\"].min()) / (df[\"DeliveryTime\"].max() - df[\"DeliveryTime\"].min())\n",
        "\n",
        "# Normalize experience and quality (higher is better)\n",
        "df_normalized[\"Experience\"] = (df[\"Experience\"] - df[\"Experience\"].min()) / (df[\"Experience\"].max() - df[\"Experience\"].min())\n",
        "df_normalized[\"QualityScore\"] = (df[\"QualityScore\"] - df[\"QualityScore\"].min()) / (df[\"QualityScore\"].max() - df[\"QualityScore\"].min())\n",
        "\n",
        "print(\"\\n=== Normalized Data ===\")\n",
        "print(df_normalized)\n",
        "\n",
        "# Step 4: Weighted Scoring\n",
        "def calculate_score(row, weights):\n",
        "    score = 0\n",
        "    for k in weights:\n",
        "        score += row[k] * weights[k]\n",
        "    return score\n",
        "\n",
        "df_normalized[\"Score\"] = df_normalized.apply(lambda row: calculate_score(row, weights), axis=1)\n",
        "\n",
        "# Step 5: Rank Bidders\n",
        "df_final = df.copy()\n",
        "df_final[\"Score\"] = df_normalized[\"Score\"]\n",
        "df_final[\"Rank\"] = df_final[\"Score\"].rank(ascending=False).astype(int)\n",
        "df_final = df_final.sort_values(by=\"Score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n=== Final Evaluation Results ===\")\n",
        "print(tabulate(df_final, headers='keys', tablefmt='fancy_grid', showindex=False))\n",
        "\n",
        "# Step 6: Visualizations\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.barplot(x='Bidder', y='Score', data=df_final, palette='viridis')\n",
        "plt.title(\"Bidder Evaluation Scores\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(df_normalized.set_index(\"Bidder\")[[\"Cost\", \"Experience\", \"QualityScore\", \"DeliveryTime\"]], annot=True, cmap='coolwarm')\n",
        "plt.title(\"Normalized Bidder Attributes\")\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Show Best Bidder\n",
        "best_bidder = df_final.iloc[0]\n",
        "print(f\"\\nüèÜ Best Bidder: {best_bidder['Bidder']} with Score: {best_bidder['Score']:.3f}\")\n",
        "\n",
        "# Step 8: Export to Excel (if desired)\n",
        "df_final.to_csv(\"bidder_evaluation_results.csv\", index=False)\n",
        "print(\"\\n‚úÖ Evaluation complete. Results saved to 'bidder_evaluation_results.csv'\")\n"
      ],
      "metadata": {
        "id": "knyhElSBxoJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Diploma Document OCR and Configuration Suite\n",
        "\n",
        "This extended Jupyter notebook provides a complete, modular, and scalable OCR-based document\n",
        "processing pipeline tailored for high-volume, multi-page diploma or academic document processing.\n",
        "It includes advanced configuration options, metadata entry, batch processing support, page-level\n",
        "navigation, similarity evaluation, and validation mechanisms to ensure extracted data is both accurate\n",
        "and properly attributed.\n",
        "\n",
        "Primary Features:\n",
        "- Accepts long, high-resolution multi-page documents (PDF or image)\n",
        "- Supports multiple OCR engines: Tesseract, DocTR (TensorFlow, PyTorch)\n",
        "- Allows metadata entry: university name, degree type, graduate name, issue date, etc.\n",
        "- Provides structured output for downstream validation or archival\n",
        "- Built-in interface for comparing OCR results to ground-truth manually entered by operator\n",
        "- Optional export to structured JSON for further processing (e.g., database or backend validation)\n",
        "\"\"\"\n",
        "\n",
        "import pytesseract\n",
        "import fitz  # PyMuPDF\n",
        "from doctr.io import DocumentFile\n",
        "from doctr.models import ocr_predictor\n",
        "from PIL import Image\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "import io\n",
        "import json\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# Section: Upload interface\n",
        "upload_widget = widgets.FileUpload(accept='.pdf,.png,.jpg,.jpeg', multiple=False)\n",
        "display(widgets.HTML(\"<h2>Diploma Document Upload</h2>\"))\n",
        "display(upload_widget)\n",
        "\n",
        "# Section: OCR Engine Selection\n",
        "ocr_selector = widgets.Dropdown(\n",
        "    options=['Tesseract', 'DocTR (TensorFlow)', 'DocTR (PyTorch)'],\n",
        "    value='DocTR (TensorFlow)',\n",
        "    description='OCR Engine:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='50%')\n",
        ")\n",
        "display(widgets.HTML(\"<h3>Select OCR Engine</h3>\"))\n",
        "display(ocr_selector)\n",
        "\n",
        "# Section: Diploma Metadata Configuration\n",
        "metadata_fields = {\n",
        "    'university_name': widgets.Text(description='University Name:'),\n",
        "    'degree_type': widgets.Text(description='Degree Type:'),\n",
        "    'graduate_name': widgets.Text(description='Graduate Name:'),\n",
        "    'graduation_date': widgets.Text(description='Graduation Date:'),\n",
        "    'document_id': widgets.Text(description='Document ID (Optional):')\n",
        "}\n",
        "display(widgets.HTML(\"<h3>Diploma Metadata Entry</h3>\"))\n",
        "for field in metadata_fields.values():\n",
        "    display(field)\n",
        "\n",
        "# OCR Execution Function\n",
        "def perform_ocr(file, engine_choice):\n",
        "    extracted_text_pages = []\n",
        "    if file.name.endswith('.pdf'):\n",
        "        doc = fitz.open(stream=file.content, filetype=\"pdf\")\n",
        "        images = [Image.open(io.BytesIO(page.get_pixmap(dpi=300).tobytes())) for page in doc]\n",
        "    else:\n",
        "        images = [Image.open(io.BytesIO(file.content))]\n",
        "\n",
        "    if engine_choice == 'Tesseract':\n",
        "        for image in images:\n",
        "            text = pytesseract.image_to_string(image)\n",
        "            extracted_text_pages.append(text)\n",
        "    else:\n",
        "        doc_input = DocumentFile.from_pdf(io.BytesIO(file.content)) if file.name.endswith('.pdf') else DocumentFile.from_images(images)\n",
        "        model = ocr_predictor(pretrained=True, assume_straight_pages=True, use_pytorch=(engine_choice == 'DocTR (PyTorch)'))\n",
        "        result = model(doc_input)\n",
        "        for page in result.pages:\n",
        "            text = '\\n'.join([block['value'] for block in page.blocks])\n",
        "            extracted_text_pages.append(text)\n",
        "\n",
        "    return extracted_text_pages\n",
        "\n",
        "# Page Navigator\n",
        "\n",
        "def show_page_navigation(pages):\n",
        "    page_selector = widgets.Dropdown(\n",
        "        options=[(f\"Page {i+1}\", i) for i in range(len(pages))],\n",
        "        description=\"Select Page:\"\n",
        "    )\n",
        "    output_area = widgets.Output()\n",
        "\n",
        "    def on_page_change(change):\n",
        "        with output_area:\n",
        "            clear_output()\n",
        "            print(pages[change['new']])\n",
        "\n",
        "    page_selector.observe(on_page_change, names='value')\n",
        "    display(widgets.HTML(\"<h3>Page Navigator</h3>\"))\n",
        "    display(page_selector)\n",
        "    display(output_area)\n",
        "    on_page_change({'new': 0})\n",
        "\n",
        "# Similarity Checker Interface\n",
        "def show_evaluation_interface(ocr_pages):\n",
        "    expected_textarea = widgets.Textarea(\n",
        "        value='',\n",
        "        placeholder='Paste the expected ground-truth result here for validation...',\n",
        "        layout=widgets.Layout(width='100%', height='150px')\n",
        "    )\n",
        "    eval_button = widgets.Button(description=\"Evaluate OCR Similarity\")\n",
        "    eval_output = widgets.Output()\n",
        "\n",
        "    def on_eval_clicked(b):\n",
        "        with eval_output:\n",
        "            clear_output()\n",
        "            combined_ocr_text = '\\n'.join(ocr_pages)\n",
        "            ratio = SequenceMatcher(None, expected_textarea.value.strip(), combined_ocr_text.strip()).ratio()\n",
        "            print(f\"OCR Similarity: {ratio:.2%}\")\n",
        "\n",
        "    eval_button.on_click(on_eval_clicked)\n",
        "    display(widgets.HTML(\"<h3>OCR Evaluation</h3>\"))\n",
        "    display(expected_textarea)\n",
        "    display(eval_button)\n",
        "    display(eval_output)\n",
        "\n",
        "# Export Metadata + OCR to JSON\n",
        "\n",
        "def export_to_json(metadata, ocr_text):\n",
        "    payload = {\n",
        "        'metadata': {k: v.value for k, v in metadata.items()},\n",
        "        'ocr_text': ocr_text\n",
        "    }\n",
        "    print(\"\\nStructured Export:\")\n",
        "    print(json.dumps(payload, indent=4))\n",
        "\n",
        "export_btn = widgets.Button(description=\"Export to JSON\")\n",
        "\n",
        "# Bind export\n",
        "if upload_widget.value:\n",
        "    file_data = list(upload_widget.value.values())[0]\n",
        "    engine_selected = ocr_selector.value\n",
        "    display(widgets.HTML(\"<h3>Running OCR - Please Wait</h3>\"))\n",
        "    ocr_results = perform_ocr(file_data, engine_selected)\n",
        "    display(widgets.HTML(\"<h3>OCR Completed</h3>\"))\n",
        "    show_page_navigation(ocr_results)\n",
        "    show_evaluation_interface(ocr_results)\n",
        "\n",
        "    def on_export_clicked(b):\n",
        "        export_to_json(metadata_fields, ocr_results)\n",
        "\n",
        "    export_btn.on_click(on_export_clicked)\n",
        "    display(export_btn)\n",
        "else:\n",
        "    display(widgets.HTML(\"<b>Please upload a document above to begin processing.</b>\"))"
      ],
      "metadata": {
        "id": "EUVsHYk_xmLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COLAB CV CONFIGURATION: HUGE MULTI-MODEL INITIALIZER\n",
        "\n",
        "# Step 1: Install and Import Dependencies\n",
        "!pip install -q timm torchvision transformers datasets opencv-python\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "import timm\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from tensorflow.keras.applications import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
        "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
        "from PIL import Image\n",
        "\n",
        "print(\"All dependencies imported!\")\n",
        "\n",
        "# Step 2: Load and Preprocess a Sample Image\n",
        "def load_sample_image(path='sample.jpg'):\n",
        "    # Download a sample image\n",
        "    if not os.path.exists(path):\n",
        "        !wget -q https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/640px-YellowLabradorLooking_new.jpg -O sample.jpg\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    return img\n",
        "\n",
        "img = load_sample_image()\n",
        "plt.imshow(img)\n",
        "plt.title(\"Sample Input Image\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Resize for all models\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "input_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "# Step 3: TensorFlow / Keras Models\n",
        "keras_models = {\n",
        "    \"VGG16\": VGG16(weights='imagenet', include_top=False),\n",
        "    \"VGG19\": VGG19(weights='imagenet', include_top=False),\n",
        "    \"ResNet50\": ResNet50(weights='imagenet', include_top=False),\n",
        "    \"ResNet101\": ResNet101(weights='imagenet', include_top=False),\n",
        "    \"InceptionV3\": InceptionV3(weights='imagenet', include_top=False),\n",
        "    \"Xception\": Xception(weights='imagenet', include_top=False),\n",
        "    \"MobileNet\": MobileNet(weights='imagenet', include_top=False),\n",
        "    \"DenseNet201\": DenseNet201(weights='imagenet', include_top=False),\n",
        "    \"NASNetMobile\": NASNetMobile(weights='imagenet', include_top=False),\n",
        "    \"EfficientNetB7\": EfficientNetB7(weights='imagenet', include_top=False)\n",
        "}\n",
        "\n",
        "def keras_model_summary():\n",
        "    for name, base_model in keras_models.items():\n",
        "        model = tf.keras.Sequential([\n",
        "            base_model,\n",
        "            GlobalAveragePooling2D(),\n",
        "            Dense(1024, activation='relu'),\n",
        "            Dense(10, activation='softmax')\n",
        "        ])\n",
        "        print(f\"Summary of {name}:\")\n",
        "        model.build((None, 224, 224, 3))\n",
        "        model.summary()\n",
        "        print(\"=\"*80)\n",
        "\n",
        "keras_model_summary()\n",
        "\n",
        "# Step 4: PyTorch Vision Models\n",
        "torch_models = {\n",
        "    \"resnet18\": torchvision.models.resnet18(pretrained=True),\n",
        "    \"resnet50\": torchvision.models.resnet50(pretrained=True),\n",
        "    \"alexnet\": torchvision.models.alexnet(pretrained=True),\n",
        "    \"squeezenet\": torchvision.models.squeezenet1_0(pretrained=True),\n",
        "    \"vgg16\": torchvision.models.vgg16(pretrained=True),\n",
        "    \"densenet\": torchvision.models.densenet161(pretrained=True),\n",
        "    \"inception\": torchvision.models.inception_v3(pretrained=True, aux_logits=False),\n",
        "    \"googlenet\": torchvision.models.googlenet(pretrained=True),\n",
        "    \"shufflenet\": torchvision.models.shufflenet_v2_x1_0(pretrained=True),\n",
        "    \"mobilenet\": torchvision.models.mobilenet_v2(pretrained=True),\n",
        "    \"resnext\": torchvision.models.resnext50_32x4d(pretrained=True),\n",
        "    \"wide_resnet\": torchvision.models.wide_resnet50_2(pretrained=True),\n",
        "    \"mnasnet\": torchvision.models.mnasnet1_0(pretrained=True),\n",
        "}\n",
        "\n",
        "for name, model in torch_models.items():\n",
        "    model.eval()\n",
        "    out = model(input_tensor)\n",
        "    print(f\"{name} output shape: {out.shape}\")\n",
        "\n",
        "# Step 5: TIMM Models\n",
        "timm_models = [\n",
        "    'vit_base_patch16_224',\n",
        "    'swin_base_patch4_window7_224',\n",
        "    'efficientnet_b3a',\n",
        "    'resnext101_32x8d',\n",
        "    'regnety_160',\n",
        "    'tf_efficientnet_b7_ns',\n",
        "    'convnext_base',\n",
        "    'beit_base_patch16_224'\n",
        "]\n",
        "\n",
        "print(\"TIMM models output shapes:\")\n",
        "for model_name in timm_models:\n",
        "    model = timm.create_model(model_name, pretrained=True)\n",
        "    model.eval()\n",
        "    out = model(input_tensor)\n",
        "    print(f\"{model_name}: {out.shape}\")\n",
        "\n",
        "# Step 6: Transformers Vision Models\n",
        "hf_models = [\n",
        "    \"google/vit-base-patch16-224\",\n",
        "    \"microsoft/resnet-50\",\n",
        "    \"facebook/deit-base-distilled-patch16-224\",\n",
        "    \"microsoft/swin-tiny-patch4-window7-224\"\n",
        "]\n",
        "\n",
        "for model_name in hf_models:\n",
        "    print(f\"Loading HuggingFace model: {model_name}\")\n",
        "    extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "    model = AutoModelForImageClassification.from_pretrained(model_name)\n",
        "\n",
        "    img_array = extractor(images=img, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output = model(**img_array)\n",
        "    print(f\"{model_name} logits shape: {output.logits.shape}\")\n",
        "\n",
        "# Step 7: Sample CV Tasks\n",
        "print(\"\\nPerforming Sample Classification with ResNet18:\")\n",
        "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
        "resnet18.eval()\n",
        "output = resnet18(input_tensor)\n",
        "prob = torch.nn.functional.softmax(output[0], dim=0)\n",
        "top5 = torch.topk(prob, 5)\n",
        "\n",
        "# Download labels\n",
        "!wget -q https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
        "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"{categories[top5.indices[i]]}: {top5.values[i].item()*100:.2f}%\")\n",
        "\n",
        "# Step 8: Object Detection with Pretrained Faster R-CNN\n",
        "print(\"\\nRunning object detection with Faster R-CNN\")\n",
        "od_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "od_model.eval()\n",
        "image_tensor = transform(img).unsqueeze(0)\n",
        "predictions = od_model(image_tensor)\n",
        "\n",
        "for i in range(len(predictions[0]['boxes'])):\n",
        "    score = predictions[0]['scores'][i].item()\n",
        "    if score > 0.5:\n",
        "        box = predictions[0]['boxes'][i].detach().numpy()\n",
        "        print(f\"Object {i}: Box={box}, Score={score:.2f}\")\n",
        "\n",
        "# Step 9: Semantic Segmentation with DeepLabV3\n",
        "print(\"\\nRunning semantic segmentation with DeepLabV3\")\n",
        "seg_model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
        "seg_model.eval()\n",
        "output = seg_model(image_tensor)['out']\n",
        "seg = output.squeeze().argmax(0).detach().cpu().numpy()\n",
        "\n",
        "plt.imshow(seg)\n",
        "plt.title(\"Segmentation Output\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDone. You have configured a huge number of CV models!\")\n"
      ],
      "metadata": {
        "id": "aX9sQ2J1xkfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bY0-J3_BQvq"
      },
      "outputs": [],
      "source": [
        "# BIDDER EVALUATION SYSTEM - Google Colab Version\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Step 1: Generate Sample Bidder Data\n",
        "np.random.seed(42)\n",
        "\n",
        "bidders = [f\"Bidder_{i+1}\" for i in range(15)]\n",
        "data = {\n",
        "    \"Bidder\": bidders,\n",
        "    \"Cost\": np.random.randint(100000, 500000, size=len(bidders)),\n",
        "    \"Experience\": np.random.randint(1, 10, size=len(bidders)),  # in years\n",
        "    \"QualityScore\": np.random.randint(60, 100, size=len(bidders)),  # out of 100\n",
        "    \"DeliveryTime\": np.random.randint(15, 60, size=len(bidders))  # in days\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"=== Raw Bidder Data ===\")\n",
        "print(df)\n",
        "\n",
        "# Step 2: Define Evaluation Weights\n",
        "# Lower cost and delivery time is better. Higher experience and quality is better.\n",
        "weights = {\n",
        "    \"Cost\": 0.4,\n",
        "    \"Experience\": 0.2,\n",
        "    \"QualityScore\": 0.3,\n",
        "    \"DeliveryTime\": 0.1\n",
        "}\n",
        "\n",
        "print(\"\\n=== Evaluation Criteria Weights ===\")\n",
        "print(weights)\n",
        "\n",
        "# Step 3: Normalize Data for Fair Comparison\n",
        "df_normalized = df.copy()\n",
        "\n",
        "# Normalize cost and delivery time by min-max (lower is better ‚Üí inverse)\n",
        "df_normalized[\"Cost\"] = 1 - (df[\"Cost\"] - df[\"Cost\"].min()) / (df[\"Cost\"].max() - df[\"Cost\"].min())\n",
        "df_normalized[\"DeliveryTime\"] = 1 - (df[\"DeliveryTime\"] - df[\"DeliveryTime\"].min()) / (df[\"DeliveryTime\"].max() - df[\"DeliveryTime\"].min())\n",
        "\n",
        "# Normalize experience and quality (higher is better)\n",
        "df_normalized[\"Experience\"] = (df[\"Experience\"] - df[\"Experience\"].min()) / (df[\"Experience\"].max() - df[\"Experience\"].min())\n",
        "df_normalized[\"QualityScore\"] = (df[\"QualityScore\"] - df[\"QualityScore\"].min()) / (df[\"QualityScore\"].max() - df[\"QualityScore\"].min())\n",
        "\n",
        "print(\"\\n=== Normalized Data ===\")\n",
        "print(df_normalized)\n",
        "\n",
        "# Step 4: Weighted Scoring\n",
        "def calculate_score(row, weights):\n",
        "    score = 0\n",
        "    for k in weights:\n",
        "        score += row[k] * weights[k]\n",
        "    return score\n",
        "\n",
        "df_normalized[\"Score\"] = df_normalized.apply(lambda row: calculate_score(row, weights), axis=1)\n",
        "\n",
        "# Step 5: Rank Bidders\n",
        "df_final = df.copy()\n",
        "df_final[\"Score\"] = df_normalized[\"Score\"]\n",
        "df_final[\"Rank\"] = df_final[\"Score\"].rank(ascending=False).astype(int)\n",
        "df_final = df_final.sort_values(by=\"Score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n=== Final Evaluation Results ===\")\n",
        "print(tabulate(df_final, headers='keys', tablefmt='fancy_grid', showindex=False))\n",
        "\n",
        "# Step 6: Visualizations\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.barplot(x='Bidder', y='Score', data=df_final, palette='viridis')\n",
        "plt.title(\"Bidder Evaluation Scores\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(df_normalized.set_index(\"Bidder\")[[\"Cost\", \"Experience\", \"QualityScore\", \"DeliveryTime\"]], annot=True, cmap='coolwarm')\n",
        "plt.title(\"Normalized Bidder Attributes\")\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Show Best Bidder\n",
        "best_bidder = df_final.iloc[0]\n",
        "print(f\"\\nüèÜ Best Bidder: {best_bidder['Bidder']} with Score: {best_bidder['Score']:.3f}\")\n",
        "\n",
        "# Step 8: Export to Excel (if desired)\n",
        "df_final.to_csv(\"bidder_evaluation_results.csv\", index=False)\n",
        "print(\"\\n‚úÖ Evaluation complete. Results saved to 'bidder_evaluation_results.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BIDDER EVALUATION SYSTEM - Google Colab Version\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Step 1: Generate Sample Bidder Data\n",
        "np.random.seed(42)\n",
        "\n",
        "bidders = [f\"Bidder_{i+1}\" for i in range(15)]\n",
        "data = {\n",
        "    \"Bidder\": bidders,\n",
        "    \"Cost\": np.random.randint(100000, 500000, size=len(bidders)),\n",
        "    \"Experience\": np.random.randint(1, 10, size=len(bidders)),  # in years\n",
        "    \"QualityScore\": np.random.randint(60, 100, size=len(bidders)),  # out of 100\n",
        "    \"DeliveryTime\": np.random.randint(15, 60, size=len(bidders))  # in days\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"=== Raw Bidder Data ===\")\n",
        "print(df)\n",
        "\n",
        "# Step 2: Define Evaluation Weights\n",
        "# Lower cost and delivery time is better. Higher experience and quality is better.\n",
        "weights = {\n",
        "    \"Cost\": 0.4,\n",
        "    \"Experience\": 0.2,\n",
        "    \"QualityScore\": 0.3,\n",
        "    \"DeliveryTime\": 0.1\n",
        "}\n",
        "\n",
        "print(\"\\n=== Evaluation Criteria Weights ===\")\n",
        "print(weights)\n",
        "\n",
        "# Step 3: Normalize Data for Fair Comparison\n",
        "df_normalized = df.copy()\n",
        "\n",
        "# Normalize cost and delivery time by min-max (lower is better ‚Üí inverse)\n",
        "df_normalized[\"Cost\"] = 1 - (df[\"Cost\"] - df[\"Cost\"].min()) / (df[\"Cost\"].max() - df[\"Cost\"].min())\n",
        "df_normalized[\"DeliveryTime\"] = 1 - (df[\"DeliveryTime\"] - df[\"DeliveryTime\"].min()) / (df[\"DeliveryTime\"].max() - df[\"DeliveryTime\"].min())\n",
        "\n",
        "# Normalize experience and quality (higher is better)\n",
        "df_normalized[\"Experience\"] = (df[\"Experience\"] - df[\"Experience\"].min()) / (df[\"Experience\"].max() - df[\"Experience\"].min())\n",
        "df_normalized[\"QualityScore\"] = (df[\"QualityScore\"] - df[\"QualityScore\"].min()) / (df[\"QualityScore\"].max() - df[\"QualityScore\"].min())\n",
        "\n",
        "print(\"\\n=== Normalized Data ===\")\n",
        "print(df_normalized)\n",
        "\n",
        "# Step 4: Weighted Scoring\n",
        "def calculate_score(row, weights):\n",
        "    score = 0\n",
        "    for k in weights:\n",
        "        score += row[k] * weights[k]\n",
        "    return score\n",
        "\n",
        "df_normalized[\"Score\"] = df_normalized.apply(lambda row: calculate_score(row, weights), axis=1)\n",
        "\n",
        "# Step 5: Rank Bidders\n",
        "df_final = df.copy()\n",
        "df_final[\"Score\"] = df_normalized[\"Score\"]\n",
        "df_final[\"Rank\"] = df_final[\"Score\"].rank(ascending=False).astype(int)\n",
        "df_final = df_final.sort_values(by=\"Score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n=== Final Evaluation Results ===\")\n",
        "print(tabulate(df_final, headers='keys', tablefmt='fancy_grid', showindex=False))\n",
        "\n",
        "# Step 6: Visualizations\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.barplot(x='Bidder', y='Score', data=df_final, palette='viridis')\n",
        "plt.title(\"Bidder Evaluation Scores\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(df_normalized.set_index(\"Bidder\")[[\"Cost\", \"Experience\", \"QualityScore\", \"DeliveryTime\"]], annot=True, cmap='coolwarm')\n",
        "plt.title(\"Normalized Bidder Attributes\")\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Show Best Bidder\n",
        "best_bidder = df_final.iloc[0]\n",
        "print(f\"\\nüèÜ Best Bidder: {best_bidder['Bidder']} with Score: {best_bidder['Score']:.3f}\")\n",
        "\n",
        "# Step 8: Export to Excel (if desired)\n",
        "df_final.to_csv(\"bidder_evaluation_results.csv\", index=False)\n",
        "print(\"\\n‚úÖ Evaluation complete. Results saved to 'bidder_evaluation_results.csv'\")\n"
      ],
      "metadata": {
        "id": "7MwVyARlxW66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BIDDER EVALUATION SYSTEM - Google Colab Version\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Step 1: Generate Sample Bidder Data\n",
        "np.random.seed(42)\n",
        "\n",
        "bidders = [f\"Bidder_{i+1}\" for i in range(15)]\n",
        "data = {\n",
        "    \"Bidder\": bidders,\n",
        "    \"Cost\": np.random.randint(100000, 500000, size=len(bidders)),\n",
        "    \"Experience\": np.random.randint(1, 10, size=len(bidders)),  # in years\n",
        "    \"QualityScore\": np.random.randint(60, 100, size=len(bidders)),  # out of 100\n",
        "    \"DeliveryTime\": np.random.randint(15, 60, size=len(bidders))  # in days\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"=== Raw Bidder Data ===\")\n",
        "print(df)\n",
        "\n",
        "# Step 2: Define Evaluation Weights\n",
        "# Lower cost and delivery time is better. Higher experience and quality is better.\n",
        "weights = {\n",
        "    \"Cost\": 0.4,\n",
        "    \"Experience\": 0.2,\n",
        "    \"QualityScore\": 0.3,\n",
        "    \"DeliveryTime\": 0.1\n",
        "}\n",
        "\n",
        "print(\"\\n=== Evaluation Criteria Weights ===\")\n",
        "print(weights)\n",
        "\n",
        "# Step 3: Normalize Data for Fair Comparison\n",
        "df_normalized = df.copy()\n",
        "\n",
        "# Normalize cost and delivery time by min-max (lower is better ‚Üí inverse)\n",
        "df_normalized[\"Cost\"] = 1 - (df[\"Cost\"] - df[\"Cost\"].min()) / (df[\"Cost\"].max() - df[\"Cost\"].min())\n",
        "df_normalized[\"DeliveryTime\"] = 1 - (df[\"DeliveryTime\"] - df[\"DeliveryTime\"].min()) / (df[\"DeliveryTime\"].max() - df[\"DeliveryTime\"].min())\n",
        "\n",
        "# Normalize experience and quality (higher is better)\n",
        "df_normalized[\"Experience\"] = (df[\"Experience\"] - df[\"Experience\"].min()) / (df[\"Experience\"].max() - df[\"Experience\"].min())\n",
        "df_normalized[\"QualityScore\"] = (df[\"QualityScore\"] - df[\"QualityScore\"].min()) / (df[\"QualityScore\"].max() - df[\"QualityScore\"].min())\n",
        "\n",
        "print(\"\\n=== Normalized Data ===\")\n",
        "print(df_normalized)\n",
        "\n",
        "# Step 4: Weighted Scoring\n",
        "def calculate_score(row, weights):\n",
        "    score = 0\n",
        "    for k in weights:\n",
        "        score += row[k] * weights[k]\n",
        "    return score\n",
        "\n",
        "df_normalized[\"Score\"] = df_normalized.apply(lambda row: calculate_score(row, weights), axis=1)\n",
        "\n",
        "# Step 5: Rank Bidders\n",
        "df_final = df.copy()\n",
        "df_final[\"Score\"] = df_normalized[\"Score\"]\n",
        "df_final[\"Rank\"] = df_final[\"Score\"].rank(ascending=False).astype(int)\n",
        "df_final = df_final.sort_values(by=\"Score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n=== Final Evaluation Results ===\")\n",
        "print(tabulate(df_final, headers='keys', tablefmt='fancy_grid', showindex=False))\n",
        "\n",
        "# Step 6: Visualizations\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.barplot(x='Bidder', y='Score', data=df_final, palette='viridis')\n",
        "plt.title(\"Bidder Evaluation Scores\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(df_normalized.set_index(\"Bidder\")[[\"Cost\", \"Experience\", \"QualityScore\", \"DeliveryTime\"]], annot=True, cmap='coolwarm')\n",
        "plt.title(\"Normalized Bidder Attributes\")\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Show Best Bidder\n",
        "best_bidder = df_final.iloc[0]\n",
        "print(f\"\\nüèÜ Best Bidder: {best_bidder['Bidder']} with Score: {best_bidder['Score']:.3f}\")\n",
        "\n",
        "# Step 8: Export to Excel (if desired)\n",
        "df_final.to_csv(\"bidder_evaluation_results.csv\", index=False)\n",
        "print(\"\\n‚úÖ Evaluation complete. Results saved to 'bidder_evaluation_results.csv'\")\n"
      ],
      "metadata": {
        "id": "-YbOGQJ7xXqV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}