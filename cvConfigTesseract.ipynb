{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bY0-J3_BQvq"
      },
      "outputs": [],
      "source": [
        "# COLAB CV CONFIGURATION: HUGE MULTI-MODEL INITIALIZER\n",
        "\n",
        "# Step 1: Install and Import Dependencies\n",
        "!pip install -q timm torchvision transformers datasets opencv-python\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "import timm\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from tensorflow.keras.applications import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
        "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
        "from PIL import Image\n",
        "\n",
        "print(\"All dependencies imported!\")\n",
        "\n",
        "# Step 2: Load and Preprocess a Sample Image\n",
        "def load_sample_image(path='sample.jpg'):\n",
        "    # Download a sample image\n",
        "    if not os.path.exists(path):\n",
        "        !wget -q https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/640px-YellowLabradorLooking_new.jpg -O sample.jpg\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    return img\n",
        "\n",
        "img = load_sample_image()\n",
        "plt.imshow(img)\n",
        "plt.title(\"Sample Input Image\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Resize for all models\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "input_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "# Step 3: TensorFlow / Keras Models\n",
        "keras_models = {\n",
        "    \"VGG16\": VGG16(weights='imagenet', include_top=False),\n",
        "    \"VGG19\": VGG19(weights='imagenet', include_top=False),\n",
        "    \"ResNet50\": ResNet50(weights='imagenet', include_top=False),\n",
        "    \"ResNet101\": ResNet101(weights='imagenet', include_top=False),\n",
        "    \"InceptionV3\": InceptionV3(weights='imagenet', include_top=False),\n",
        "    \"Xception\": Xception(weights='imagenet', include_top=False),\n",
        "    \"MobileNet\": MobileNet(weights='imagenet', include_top=False),\n",
        "    \"DenseNet201\": DenseNet201(weights='imagenet', include_top=False),\n",
        "    \"NASNetMobile\": NASNetMobile(weights='imagenet', include_top=False),\n",
        "    \"EfficientNetB7\": EfficientNetB7(weights='imagenet', include_top=False)\n",
        "}\n",
        "\n",
        "def keras_model_summary():\n",
        "    for name, base_model in keras_models.items():\n",
        "        model = tf.keras.Sequential([\n",
        "            base_model,\n",
        "            GlobalAveragePooling2D(),\n",
        "            Dense(1024, activation='relu'),\n",
        "            Dense(10, activation='softmax')\n",
        "        ])\n",
        "        print(f\"Summary of {name}:\")\n",
        "        model.build((None, 224, 224, 3))\n",
        "        model.summary()\n",
        "        print(\"=\"*80)\n",
        "\n",
        "keras_model_summary()\n",
        "\n",
        "# Step 4: PyTorch Vision Models\n",
        "torch_models = {\n",
        "    \"resnet18\": torchvision.models.resnet18(pretrained=True),\n",
        "    \"resnet50\": torchvision.models.resnet50(pretrained=True),\n",
        "    \"alexnet\": torchvision.models.alexnet(pretrained=True),\n",
        "    \"squeezenet\": torchvision.models.squeezenet1_0(pretrained=True),\n",
        "    \"vgg16\": torchvision.models.vgg16(pretrained=True),\n",
        "    \"densenet\": torchvision.models.densenet161(pretrained=True),\n",
        "    \"inception\": torchvision.models.inception_v3(pretrained=True, aux_logits=False),\n",
        "    \"googlenet\": torchvision.models.googlenet(pretrained=True),\n",
        "    \"shufflenet\": torchvision.models.shufflenet_v2_x1_0(pretrained=True),\n",
        "    \"mobilenet\": torchvision.models.mobilenet_v2(pretrained=True),\n",
        "    \"resnext\": torchvision.models.resnext50_32x4d(pretrained=True),\n",
        "    \"wide_resnet\": torchvision.models.wide_resnet50_2(pretrained=True),\n",
        "    \"mnasnet\": torchvision.models.mnasnet1_0(pretrained=True),\n",
        "}\n",
        "\n",
        "for name, model in torch_models.items():\n",
        "    model.eval()\n",
        "    out = model(input_tensor)\n",
        "    print(f\"{name} output shape: {out.shape}\")\n",
        "\n",
        "# Step 5: TIMM Models\n",
        "timm_models = [\n",
        "    'vit_base_patch16_224',\n",
        "    'swin_base_patch4_window7_224',\n",
        "    'efficientnet_b3a',\n",
        "    'resnext101_32x8d',\n",
        "    'regnety_160',\n",
        "    'tf_efficientnet_b7_ns',\n",
        "    'convnext_base',\n",
        "    'beit_base_patch16_224'\n",
        "]\n",
        "\n",
        "print(\"TIMM models output shapes:\")\n",
        "for model_name in timm_models:\n",
        "    model = timm.create_model(model_name, pretrained=True)\n",
        "    model.eval()\n",
        "    out = model(input_tensor)\n",
        "    print(f\"{model_name}: {out.shape}\")\n",
        "\n",
        "# Step 6: Transformers Vision Models\n",
        "hf_models = [\n",
        "    \"google/vit-base-patch16-224\",\n",
        "    \"microsoft/resnet-50\",\n",
        "    \"facebook/deit-base-distilled-patch16-224\",\n",
        "    \"microsoft/swin-tiny-patch4-window7-224\"\n",
        "]\n",
        "\n",
        "for model_name in hf_models:\n",
        "    print(f\"Loading HuggingFace model: {model_name}\")\n",
        "    extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "    model = AutoModelForImageClassification.from_pretrained(model_name)\n",
        "\n",
        "    img_array = extractor(images=img, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output = model(**img_array)\n",
        "    print(f\"{model_name} logits shape: {output.logits.shape}\")\n",
        "\n",
        "# Step 7: Sample CV Tasks\n",
        "print(\"\\nPerforming Sample Classification with ResNet18:\")\n",
        "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
        "resnet18.eval()\n",
        "output = resnet18(input_tensor)\n",
        "prob = torch.nn.functional.softmax(output[0], dim=0)\n",
        "top5 = torch.topk(prob, 5)\n",
        "\n",
        "# Download labels\n",
        "!wget -q https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
        "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"{categories[top5.indices[i]]}: {top5.values[i].item()*100:.2f}%\")\n",
        "\n",
        "# Step 8: Object Detection with Pretrained Faster R-CNN\n",
        "print(\"\\nRunning object detection with Faster R-CNN\")\n",
        "od_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "od_model.eval()\n",
        "image_tensor = transform(img).unsqueeze(0)\n",
        "predictions = od_model(image_tensor)\n",
        "\n",
        "for i in range(len(predictions[0]['boxes'])):\n",
        "    score = predictions[0]['scores'][i].item()\n",
        "    if score > 0.5:\n",
        "        box = predictions[0]['boxes'][i].detach().numpy()\n",
        "        print(f\"Object {i}: Box={box}, Score={score:.2f}\")\n",
        "\n",
        "# Step 9: Semantic Segmentation with DeepLabV3\n",
        "print(\"\\nRunning semantic segmentation with DeepLabV3\")\n",
        "seg_model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
        "seg_model.eval()\n",
        "output = seg_model(image_tensor)['out']\n",
        "seg = output.squeeze().argmax(0).detach().cpu().numpy()\n",
        "\n",
        "plt.imshow(seg)\n",
        "plt.title(\"Segmentation Output\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDone. You have configured a huge number of CV models!\")\n"
      ]
    }
  ]
}